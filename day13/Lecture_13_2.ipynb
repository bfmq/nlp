{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(seq_k,seq_q):\n",
    "    \n",
    "    len_q = seq_q.size(1)\n",
    "    pad_mask = seq_k.eq(0)\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1,len_q,-1)\n",
    "    \n",
    "    return pad_mask\n",
    "\n",
    "def sequence_mask(seq):\n",
    "    batch_size,seq_len = seq.size()\n",
    "    mask = torch.triu(torch.ones((seq_len,seq_len),dtype=torch.uint8),\n",
    "                     diagonal = 1)\n",
    "    mask = mask.unsqueeze(0).expand(batch_size,-1,-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3,0,0,0]])\n",
    "b = torch.tensor([[1,2,3,4,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, dropout=0.0):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len,d_model)\n",
    "        \n",
    "        position = torch.arange(0.,max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.,d_model,2)*-(math.log(10000.0)/d_model))\n",
    "        \n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,attention_dropout = 0.0):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "    \n",
    "    def forward(self,q,k,v,scale=None,attn_mask = None):\n",
    "        attention = torch.matmul(q, k.transpose(-2,-1)) # q*k.T\n",
    "        \n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            attention = attention.masked_fill_(attn_mask,-np.inf)\n",
    "        \n",
    "        attention = self.softmax(attention)\n",
    "        attention = self.dropout(attention)\n",
    "        context = torch.matmul(attention,v)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=512,num_heads=8,dropout=0.0):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.dim_per_head = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_k = nn.Linear(d_model,d_model)\n",
    "        self.linear_v = nn.Linear(d_model,d_model)\n",
    "        self.linear_q = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        self.dot_product_attention = ScaledDotProductAttention(dropout)\n",
    "        self.linear_final = nn.Linear(d_model,d_model) #全链接层\n",
    "        self.norm = nn.LayerNorm(d_model) \n",
    "    \n",
    "    def forward(self,keys,values,queries, attn_mask = None):\n",
    "        \n",
    "        residul = queries # residual connection\n",
    "        batch_size = keys.size(0)\n",
    "        \n",
    "        keys = self.linear_k(keys)\n",
    "        values = self.linear_v(values)\n",
    "        queries = self.linear_q(queries)\n",
    "        \n",
    "        keys = keys.view(batch_szie, -1, self.num_heads, self.dim_per_head).transpose(1,2) #[batch_size, num_heads,length,d_model]\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1,2)\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1,self.num_heads,1,1)\n",
    "        \n",
    "        scale = (keys.size(-1)) ** -0.5\n",
    "        \n",
    "        context = self.dot_product_attention(queries, keys, values, scale, attn_mask)\n",
    "        \n",
    "        context = context.transpose(1,2).contiguous() \\\n",
    "                  .view(batch_size, -1, self.num_heads * self.dim_per_head)\n",
    "        \n",
    "        return self.norm(residul +  self.linear_final(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalWiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=512,ffn_dim=2048,dropout=0.0):\n",
    "        super(PositionalWiseFeedForward,self).__init__()\n",
    "        self.w1 = nn.Linear(d_model, ffn_dim)\n",
    "        self.w2 = nn.Linear(ffn_dim,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self.w2(F.relu(self.w1(x)))\n",
    "        return self.norm(x + self.dropout(output)) #residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Layer(nn.Module):  #one block\n",
    "    \n",
    "    def __init__(self, d_model = 512, num_heads = 8,\n",
    "                ffn_dim = 2048, dropout = 0.0):\n",
    "        super(Encoder_Layer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_mode,num_heads,dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim,dropout)\n",
    "    \n",
    "    def forward(self, x, attn_mask = None):\n",
    "        context = self.attention(x,x,x,attn_mask)\n",
    "        output = self.feed_forward(context)\n",
    "        return output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len,\n",
    "                 num_layers=6,d_model=512,num_heads=8,\n",
    "                 ffn_dim=2048,dropout = 0.0):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model,num_heads,ffn_dim,dropout)\n",
    "                                            for _ in range(num_layers)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model,max_seq_len,dropout)\n",
    "    \n",
    "    def forward(self,x,seq_embedding):\n",
    "        embedding = seq_embedding(x)\n",
    "        output = self.pos_embedding(embedding)\n",
    "        \n",
    "        self_attention_mask = padding_mask(x.x)\n",
    "        \n",
    "        for encoder in self.encoder_layers:\n",
    "            output = encoder(output,self_attention_mask)\n",
    "        \n",
    "        return self.norm(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads = 8,\n",
    "                ffn_dim=2048,dropout=0.0):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model,num_heads,dropout)\n",
    "        self.feed_forward = PositionalWiseFeedForward(d_model, ffn_dim, dropout)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask = None, context_attn_mask=None):\n",
    "        \n",
    "        dec_output = self.attention(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "        \n",
    "        dec_output = self.attention(enc_outputs, enc_outputs, dec_output, context_attn_mask)\n",
    "        \n",
    "        dec_output = self.feed_forward(dec_output)\n",
    "        \n",
    "        return dec_output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,max_seq_len,num_layers = 6,\n",
    "                d_model = 512, num_heads = 8, ffn_dim = 2048,\n",
    "                dropout = 0.0):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "        [DecoderLayer(d_model,num_heads,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.seq_embedding = nn.Embedding(vocab_size,d_model,padding_idx = 0)\n",
    "        self.pos_embedding = PositionalEncoding(d_model,max_seq_len)\n",
    "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,inputs,enc_out,seq_embedding,context_attn_mask=None):\n",
    "        embedding = seq_embedding(inputs)\n",
    "        output = embedding + self.pos_embedding(embedding)\n",
    "        \n",
    "        self_attention_padding_mask = padding_max(inputs,inputs)\n",
    "        seq_mask = sequence_mask(inputs)\n",
    "        self_attn_mask = torch.gt((self_attention_padding_mask + seq_mask),0)\n",
    "        \n",
    "        for decoder in self.decoder_layers:\n",
    "            output = decoder(output, enc_out, self_attn_mask, context_attn_mask)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                max_len,\n",
    "                num_layers = 6,\n",
    "                stack_layers = 6,\n",
    "                d_model = 512,\n",
    "                num_heads = 8,\n",
    "                ffn_dim = 2048,\n",
    "                dropout = 0.2):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size,max_len,num_layers,d_model,num_heads,ffn_dim,dropout)\n",
    "        self.decoder = Decoder(vocab_size,max_len,num_layers,d_model,num_heads,ffn_dim,dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        #self.linear = nn.Linear(d_model,vocab_size,bias=False)\n",
    "        #self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self,src_seq,dec_tgt):\n",
    "        context_attn_mask_dec = padding_mask(dec_tgt,src_seq)\n",
    "        \n",
    "        en_output = self.encoder(src_seq,self.embedding)\n",
    "        dec_output = self.decoder(dec_tgt,en_output,self.embedding,context_attn_mask_dec)\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
